{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18a85cde",
   "metadata": {},
   "source": [
    "# RAG For Mantine Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "911749c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc20f0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.embeddings.google_genai import GoogleGenAIEmbedding\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48a54179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate pg_vector database session\n",
    "from rag_service.db import DatabaseManager\n",
    "from rag_service.models import Document, Chunk\n",
    "\n",
    "local_session = DatabaseManager.get_session_factory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0629a468",
   "metadata": {},
   "source": [
    "## Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13818b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag_service.pipeline.document_loader import load_corpus\n",
    "\n",
    "# Create document node object\n",
    "ROOT = Path(\"../documents\").resolve()\n",
    "SOURCE = \"mantine_docs\"\n",
    "\n",
    "documents = load_corpus(source=SOURCE, root=ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "926f9aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import asyncio\n",
    "from llama_index.core.bridge.pydantic import ConfigDict\n",
    "\n",
    "\n",
    "class RateLimitedGeminiEmbedding(GoogleGenAIEmbedding):\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True, extra=\"allow\")\n",
    "    def __init__(self, *args, sleep_s=1.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.sleep_s = sleep_s\n",
    "\n",
    "    async def aget_text_embedding_batch(self, texts, show_progress=True, **kwargs):\n",
    "        embeddings = []\n",
    "        for i in range(0, len(texts), self.embed_batch_size):\n",
    "            batch = texts[i:i + self.embed_batch_size]\n",
    "            await asyncio.sleep(self.sleep_s)  # throttle per batch\n",
    "            embeddings.extend(await self._aget_text_embeddings(batch))\n",
    "        return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eea0d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Google Gemini Embedding model\n",
    "from google.genai.types import EmbedContentConfig\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "EMBEDDING_DIM = os.getenv(\"EMBEDDING_DIM\")\n",
    "embedding_model = RateLimitedGeminiEmbedding(\n",
    "    model_name=\"gemini-embedding-001\", \n",
    "    api_key=GEMINI_API_KEY,\n",
    "    embedding_config=EmbedContentConfig(output_dimensionality=int(EMBEDDING_DIM)),\n",
    "    embed_batch_size=99,\n",
    "    timeout=60,\n",
    "    sleep_s=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f0ef36",
   "metadata": {},
   "source": [
    "### Instantiate Chunkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9073f52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 2000\n",
    "CHUNK_OVERLAP = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24de1f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag_service.pipeline.mantine_markdown_parser import MantineMarkdownChunker\n",
    "\n",
    "mantine_parser = MantineMarkdownChunker(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fe16fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "\n",
    "splitter = TokenTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=80,\n",
    "    separator=\"\\n\\n\",\n",
    "    backup_separators=[\"\\n\", \" \"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4091c4df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mantine_docs::mantine-llms-full.txt'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mantine_documentation = documents[0]\n",
    "mantine_documentation.doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9a752e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ingestion Pipeline\n",
    "from rag_service.pipeline.ingestion import IngestPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abd8c39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_pipeline_custom = IngestPipeline(\n",
    "    chunker_transform=mantine_parser,\n",
    "    embedding_model=embedding_model,\n",
    "    session_factory=local_session\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdfc1a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_pipeline_fixed = IngestPipeline(\n",
    "    chunker_transform=splitter,\n",
    "    embedding_model=embedding_model,\n",
    "    session_factory=local_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a680a7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingested 2902 chunks for source mantine_docs\n"
     ]
    }
   ],
   "source": [
    "res_custom = await ingest_pipeline_custom.ingest_documents(\n",
    "    documents=[mantine_documentation],\n",
    "    source=SOURCE,\n",
    "    title=\"Mantine Documentation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24ce0ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/projects/RAG_service/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:02<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingested 1505 chunks for source mantine_docs\n"
     ]
    }
   ],
   "source": [
    "res_fixed = await ingest_pipeline_fixed.ingest_documents(\n",
    "    documents=[mantine_documentation],\n",
    "    source=SOURCE,\n",
    "    title=\"Mantine Documentation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5725300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document_id': UUID('673d595d-6638-442a-b898-cd42c051c547'),\n",
       " 'doc_row': Document(source='mantine_docs', title='Mantine Documentation', doc_metadata={'n_nodes': 2902}, embedding_model='gemini-embedding-001', id=UUID('673d595d-6638-442a-b898-cd42c051c547'), created_at=datetime.datetime(2026, 2, 1, 5, 35, 3, 799329, tzinfo=datetime.timezone.utc)),\n",
       " 'n_chunks': 2902}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97f3f5fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document_id': UUID('6286e313-c8bb-454a-99cb-3ab5aa2a1f46'),\n",
       " 'doc_row': Document(source='mantine_docs', title='Mantine Documentation', doc_metadata={'n_nodes': 1505}, embedding_model='gemini-embedding-001', id=UUID('6286e313-c8bb-454a-99cb-3ab5aa2a1f46'), created_at=datetime.datetime(2026, 2, 1, 7, 37, 16, 830517, tzinfo=datetime.timezone.utc)),\n",
       " 'n_chunks': 1505}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d44781",
   "metadata": {},
   "source": [
    "## Retrieval & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f861681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class QueryItem(BaseModel):\n",
    "    id: str\n",
    "    category: str\n",
    "    difficulty: int\n",
    "    text: str\n",
    "    tags: list[str] = Field(default_factory=list)\n",
    "    \n",
    "class RetrievalHit(BaseModel):\n",
    "    query_id: str\n",
    "    query_text: str\n",
    "    run_name: str\n",
    "    param_value: int\n",
    "    rank: int\n",
    "    dist: float\n",
    "    chunk_id: str\n",
    "    chunk_text: str\n",
    "    \n",
    "class JudgementLabel(BaseModel):\n",
    "    query_id: str\n",
    "    chunk_id: str\n",
    "    relevance: int         \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5eb1ab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "query_fpath = Path(\"../evaluation/queries_b.jsonl\")\n",
    "queries = [json.loads(line) for line in query_fpath.read_text().splitlines() if line.strip()]\n",
    "query_items = [QueryItem.model_validate(q) for q in queries]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15553fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import select, text\n",
    "\n",
    "\n",
    "async def run_retrieval(\n",
    "    *,\n",
    "    queries: list[QueryItem],\n",
    "    embedding_model,\n",
    "    ef_search_values: list[int],\n",
    "    k: int = 15,\n",
    ") -> list[RetrievalHit]:\n",
    "    query_embeds = {\n",
    "        q.id: embedding_model.get_query_embedding(q.text)\n",
    "        for q in queries\n",
    "    }\n",
    "\n",
    "    hits: list[RetrievalHit] = []\n",
    "\n",
    "    async with local_session() as session:\n",
    "        for ef in ef_search_values:\n",
    "            run_name = f\"hnsw_ef{ef}_k{k}\"\n",
    "\n",
    "            for q in queries:\n",
    "                q_emb = query_embeds[q.id]\n",
    "                dist = Chunk.embedding.cosine_distance(q_emb).label(\"dist\")\n",
    "\n",
    "                stmt = (\n",
    "                    select(Chunk.id.label(\"chunk_id\"), Chunk.content.label(\"chunk_text\"), dist)\n",
    "                    .join(Document, Document.id == Chunk.document_id)\n",
    "                    .where(Document.source == SOURCE)          # or .where(Chunk.document_id == doc_id)\n",
    "                    .order_by(dist)\n",
    "                    .limit(k)\n",
    "                )\n",
    "\n",
    "                async with session.begin():  # needed for SET LOCAL\n",
    "                    await session.execute(\n",
    "                        text(f\"SET LOCAL hnsw.ef_search = {ef}\"),\n",
    "                    )\n",
    "                    rows = (await session.execute(stmt)).mappings().all()\n",
    "\n",
    "                for rank, r in enumerate(rows, start=1):\n",
    "                    hits.append(\n",
    "                        RetrievalHit(\n",
    "                            query_id=q.id,  \n",
    "                            query_text=q.text,\n",
    "                            run_name=run_name,\n",
    "                            param_value=ef,\n",
    "                            rank=rank,\n",
    "                            dist=float(r[\"dist\"]),\n",
    "                            chunk_id=str(r[\"chunk_id\"]),\n",
    "                            chunk_text=r[\"chunk_text\"],\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "    return hits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5995126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ef_search_values = [50]\n",
    "\n",
    "retrieved_hits = await run_retrieval(\n",
    "    queries=query_items,\n",
    "    embedding_model=embedding_model,\n",
    "    ef_search_values=ef_search_values,\n",
    "    k=15\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aea80de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalHit(query_id='eval2_q009', query_text='How do I set up MantineProvider in Next.js App Router (app/layout.tsx)?', run_name='hnsw_ef50_k15', param_value=50, rank=1, dist=0.23000172151354092, chunk_id='3726676e-e6fe-46bb-9d84-b14cbe5bdba1', chunk_text='const theme = createTheme({\\n  /** Put your mantine theme override here */\\n});\\n\\nexport default function App({ Component, pageProps }: AppProps) {\\n  return (\\n    <MantineProvider theme={theme}>\\n      <Component {...pageProps} />\\n    </MantineProvider>\\n  );\\n}\\n```\\n\\nCreate `pages/_document.tsx` file with [ColorSchemeScript](https://mantine.dev/theming/color-schemes) component.\\nNote that it is required even if you use only one color scheme in your application.\\n\\n```tsx\\nimport { Head, Html, Main, NextScript } from \\'next/document\\';\\nimport { ColorSchemeScript, mantineHtmlProps } from \\'@mantine/core\\';\\n\\nexport default function Document() {\\n  return (\\n    <Html lang=\"en\" {...mantineHtmlProps}>\\n      <Head>\\n        <ColorSchemeScript defaultColorScheme=\"auto\" />\\n      </Head>\\n      <body>\\n        <Main />\\n        <NextScript />\\n      </body>\\n    </Html>\\n  );\\n}\\n```\\n\\nAll set! Start development server:\\n\\n```bash\\nnpm run dev\\n```\\n\\n## Setup with app router\\n\\nAdd [MantineProvider](https://mantine.dev/theming/mantine-provider), [ColorSchemeScript](https://mantine.dev/theming/color-schemes)\\nand styles imports to the `app/layout.tsx` file:\\n\\n```tsx\\n// Import styles of packages that you\\'ve installed.\\n// All packages except `@mantine/hooks` require styles imports\\nimport \\'@mantine/core/styles.css\\';\\n\\nimport { ColorSchemeScript, MantineProvider, mantineHtmlProps } from \\'@mantine/core\\';\\n\\nexport const metadata = {\\n  title: \\'My Mantine app\\',\\n  description: \\'I have followed setup instructions carefully\\',\\n};\\n\\nexport default function RootLayout({\\n  children,\\n}: {\\n  children: React.ReactNode;\\n}) {\\n  return (\\n    <html lang=\"en\" {...mantineHtmlProps}>\\n      <head>\\n        <ColorSchemeScript />\\n      </head>\\n      <body>\\n        <MantineProvider>{children}</MantineProvider>\\n      </body>\\n    </html>\\n  );\\n}\\n```\\n\\nAll set! Start development server:\\n\\n```bash\\nnpm run dev\\n```')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_hits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "445ebed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedupe_hits_best(hits: list[RetrievalHit]) -> list[RetrievalHit]:\n",
    "    best: dict[tuple[str, str], RetrievalHit] = {}\n",
    "    for h in hits:\n",
    "        key = (h.query_id, str(h.chunk_id))\n",
    "        if key not in best or h.dist < best[key].dist:\n",
    "            best[key] = h\n",
    "    return list(best.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fae4f764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "outpath = \"../evaluation/retrieval_mantine_fixed_chunk_label2.csv\"\n",
    "\n",
    "def export_hits_to_csv(hits: list[RetrievalHit], out_path: str) -> str:\n",
    "    path = Path(out_path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    deduped_hits = dedupe_hits_best(hits)\n",
    "\n",
    "    rows = []\n",
    "    for h in deduped_hits:\n",
    "        d = h.model_dump()          \n",
    "        d[\"relevance\"] = \"\"         \n",
    "        rows.append(d)\n",
    "\n",
    "    if not rows:\n",
    "        path.write_text(\"\", encoding=\"utf-8\")\n",
    "        return str(path)\n",
    "\n",
    "    with path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=rows[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "\n",
    "    return str(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0223cd24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../evaluation/retrieval_mantine_fixed_chunk_label2.csv'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = export_hits_to_csv(retrieved_hits, outpath)\n",
    "path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-service-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
